# Function to divide multiple tasks of a predicted length over multiple cores

# cal_tasks should be a data.table with at least a "Lakes" column
# use_cores should be an integer

# You need to have the file file.path(folder_root, folder_test_result, "Runtime_Equation.csv")
# which is generated by the script Runtime_dependency.R


divide_tasks_over_cores = function(cal_tasks, num_cores){
  # I did a test using GOTM to predict runtime from number of layers and length of simulation
  # Results are in this dataset
  df_runtime_pred = fread(file.path(folder_root, folder_test_result, "Runtime_Equation.csv"))
  
  # Get number of layers and length of simulation for each lake, and predict runtime 
  df_runtime = suppressWarnings({data.table(Lakes = unique(cal_tasks[, Lakes]),
                                            Layers = as.numeric(),
                                            Duration = as.numeric(),
                                            Runtime = as.numeric())}) 
  
  # Note that number of layers is only for GOTM and Simstrat. However, expectation is that GLM and FLake might
  # have a similar dependence with lake depth and number of depth for which to create output
  # (also, GOTM and especially Simstrat tend to be slowest, so most relevant bottleneck)
  
  # Fill in number of layers and runtime
  for(i in seq_len(nrow(df_runtime))){
    lakename = df_runtime[i, Lakes]
    
    df_hyps = fread(file.path(folder_root, folder_data, lakename, "gfdl-esm2m/calibration/hypsograph.csv"))
    df_obs = fread(file.path(folder_root, folder_data, lakename, "gfdl-esm2m/calibration/obs_wtemp.csv"))
    
    max_depth = max(df_hyps[, Depth_meter])
    layer_thickness = get_output_resolution(max_depth)
    num_layers = ceiling(max_depth / layer_thickness)
    
    duration = as.numeric(difftime(df_obs[.N, datetime], df_obs[1L, datetime], units = "days"))
    duration = duration / 365
    
    df_runtime[i, `:=`(Layers = num_layers,
                       Duration = duration)]
  }
  
  df_runtime[, Runtime := df_runtime_pred[1L, intercept] +
               df_runtime_pred[1L, slope_layers] * Layers +
               df_runtime_pred[1L, slope_duration] * Duration +
               df_runtime_pred[1L, slope_interaction] * Layers * Duration]
  
  cal_tasks = merge(df_runtime, cal_tasks, by = "Lakes")
  
  # Now try to divide this as efficiently as possible over the available cores
  cal_tasks[, Core := as.numeric(NA)]
  ls_tasks = lapply(seq_len(use_cores), function(x) 0)
  setorder(cal_tasks, -Runtime)
  
  # Loop over tasks and enter the row number of the task
  for(i in seq_len(nrow(cal_tasks))){
    most_free_core = which.min(sapply(ls_tasks, sum))
    ls_tasks[[most_free_core]] = ls_tasks[[most_free_core]] + cal_tasks[i, Runtime]
    cal_tasks[i, Core := most_free_core]
  }
  
  return(cal_tasks)
  
}
